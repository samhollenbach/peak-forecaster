import datetimeimport tensorflowimport pandas as pdfrom savings import SavingsCalculatorimport osfrom pprint import pprintimport numpy as npimport multiprocessing as mpfrom peak_forecaster import load, process, operate, display, networkdef do_network_training(train_data_x, train_data_y, test_data_x,                        train_type='baseline'):    """    :param train_data_x:    :param train_data_y:    :param test_data_x:    :param train_type:    :return:    """    print(f"Starting training for: {train_type}")    # Create a network to predict peaks    if train_type == 'baseline':        net = network.Network({'activation': 'relu',                               'layers': 2,                               'learning_rate': 0.001,                               'loss_positive_scalar': 5.0,                               'nodes': [16, 4],                               'optimizer':                                   tensorflow.keras.optimizers.RMSprop})    else:        net = network.Network({'activation': 'relu',                               'layers': 2,                               'learning_rate': 0.001,                               'loss_positive_scalar': 0.8,                               'nodes': [16, 4],                               'optimizer':                                   tensorflow.keras.optimizers.RMSprop})    net.build_model(len(train_data_x[0]), len(train_data_y[0]))    net.train_model(train_data_x, train_data_y)    return net.model.predict(test_data_x)def get_forecast_predictions(feature_set, is_test):    """    :param dict feature_set: Dictionary containing X/Y feature lists for each        type of feature set (baseline, crs, etc.) TODO: On-Peak/NC feature types    :param is_test:    :return:    """    predictions_all = []    for feature_type in feature_set:        features_x, features_y = feature_set[feature_type]        train_x = np.array([list(f) for f, t in zip(features_x, is_test) if not t])        train_y = np.array([list(f) for f, t in zip(features_y, is_test) if not t])        test_x = np.array([list(f) for f, t in zip(features_x, is_test) if t])        predictions_all.append(do_network_training(train_x, train_y, test_x, train_type=feature_type))    return predictions_alldef get_perfect_predictions(test_data, prediction_periods=('Non-Coincident', 'On-Peak')):    """    :param test_data:    :param prediction_periods:    :return:    """    predictions = {}    predictions_crs = {}    for period in prediction_periods:        predictions_per = []        for t in test_data:            if period == 'Non-Coincident':                predictions_per.append([t['building_baseline'].max()])            else:                # Get period max                per_df = t.loc[t['period'] == period]['building_baseline']                # If period is not present, skip                if per_df.empty:                    predictions_per.append([np.nan])                else:                    predictions_per.append([per_df.max()])        predictions[period] = predictions_per        predictions_crs_per = []        for tdata, mb in zip(test_data, predictions_per):            max_baseline = mb[0]            if np.isnan(max_baseline):                predictions_crs_per.append([np.nan])            else:                crs_val = tdata.loc[                    tdata['building_baseline'] == max_baseline]                predictions_crs_per.append(                    [crs_val['crs_baseline'].values[0]])        predictions_crs[period] = predictions_crs_per    return predictions, predictions_crsdef run_operator_base(base_data, site, start, end, predictions, predictions_crs, derating_factors=None, output_suffix='StandardOp'):    """    :param base_data:    :param site:    :param start:    :param end:    :param predictions:    :param predictions_crs:    :param derating_factors:    :return:    """    print("\nStart: ", start)    print("End: ", end, '\n')    # Split Data    start_date = pd.to_datetime(start).date()    end_date = pd.to_datetime(end).date()    train_data, test_data, is_test = process.train_test_split(base_data,                                                              test_start_date=start_date,                                                              test_end_date=end_date,                                                              return_is_test=True)    # Define standard operation window    operator = operate.StandardOperator(site, start, end)    # Define daily testing window    tstart = datetime.time(10, 0)    tend = datetime.time(23, 59)    if derating_factors is None:        derating_factors = {            'Non-Coincident': 1.0,            'On-Peak': 1.0,            'Mid-Peak': 1.0        }    # Run standard operation    targets = operator.run_standard_operation(tstart, tend, test_data,                                              predictions=predictions,                                              crs_predictions=predictions_crs,                                              dynamic_precool=False,                                              threshold_derating=derating_factors,                                              start_soc=0.5,                                              start_nth_threshold=None)    # Save results    path = f'output/{site}_{output_suffix}'    if not os.path.exists(path):        os.makedirs(path)    targets.to_pickle(f'{path}/LTSB_EBM_{site}_{start.split(" ")[0]}_{end.split(" ")[0]}.pickle')    # Calculate result savings    mapping_overrides = {'baseline': 'building_baseline',                         'actual': 'target'}    sc = SavingsCalculator('pge_e19_2019', mapping_overrides=mapping_overrides)    breakdown = sc.calculate_savings_breakdown(        targets, group_over='season', total_energy=True,        breakdown_energy=False)['actual_savings_breakdown']['all']    return targets, breakdowndef run_operator_perfect(base_data, site, start, end, derating_factors=None,                         prediction_periods=('Non-Coincident',), output_suffix='StandardOp_perfect'):    """    :param test_data:    :param base_data:    :param site:    :param start:    :param end:    :param derating_factors:    :param prediction_periods:    :return:    """    start_date = pd.to_datetime(start).date()    end_date = pd.to_datetime(end).date()    train_data, test_data = process.train_test_split(        base_data, test_start_date=start_date, test_end_date=end_date)    predictions, predictions_crs = get_perfect_predictions(test_data, prediction_periods)    return run_operator_base(base_data, site, start, end,                             predictions, predictions_crs,                             derating_factors, output_suffix=output_suffix)def run_operator_forecast(base_data, site, start, end, feature_set, derating_factors=None,                          prediction_periods=('Non-Coincident',), output_suffix='StandardOp_forecast'):    """    :param feature_set:    :param is_test:    :param base_data:    :param site:    :param start:    :param end:    :param derating_factors:    :param prediction_periods: (Disabled) TODO: Forecaster doesn't support predictions for more than NC period yet    :return:    """    start_date = pd.to_datetime(start).date()    end_date = pd.to_datetime(end).date()    train_data, test_data, is_test = process.train_test_split(base_data, test_start_date=start_date, test_end_date=end_date, return_is_test=True)    predictions, predictions_crs = get_forecast_predictions(feature_set, is_test)    return run_operator_base(base_data, site, start, end,                             predictions, predictions_crs,                             derating_factors, output_suffix=output_suffix)def run_in_parallel(func, *args, runs=1, processes=1):    """    :param func:    :param args:    :param runs:    :param processes:    :return:    """    if processes == -1:        processes = mp.cpu_count()    with mp.Pool(processes=processes) as pool:        print("Started Pool...")        formatted_args = []        for arg in args:            if (isinstance(arg, list) or isinstance(arg, tuple)) and len(arg) == runs:                formatted_args.append(arg)            else:                farg = [arg] * runs                formatted_args.append(farg)        starmap_args = zip(*formatted_args)        results = pool.starmap(func, starmap_args)        return resultsdef run_operator(site, time_ranges, derating_factors=None, processes=1, perfect_forecast=True, solar=False,  prediction_periods=('Non-Coincident',), verbose=True, plots=False):    #############    # Load Data #    #############    base_data = load.load_all_data([site])    base_data = base_data.reset_index(drop=True)    if verbose:        print("\n##### Finished Loading Data ##### \n")        print("NaNs:")        print(base_data.isna().sum())        print("Stats:")        print(base_data.describe().transpose())        print("Tail:")        print(base_data.tail())    if plots:        display.show(base_data)    output_suffix = 'StandardOp'    solar_suffix = '_solar' if solar else '_nonsolar'    output_suffix += solar_suffix    predict_suffix = '_perfect' if perfect_forecast else '_forecasted'    output_suffix += predict_suffix    #########################    # Run Operator: Perfect #    #########################    if perfect_forecast:        results = run_in_parallel(run_operator_perfect, # function call                                  base_data, # function args                                  site,                                  [start for start, _ in time_ranges],                                  [end for _, end in time_ranges],                                  derating_factors,                                  prediction_periods,                                  output_suffix,                                  runs=len(time_ranges),                                  processes=processes)    ############################    # Run Operator: Forecasted #    ############################    else:        features_x, features_y = process.extract_features(            base_data, y_value='building_baseline')        features_x_crs, features_y_crs = process.extract_features(            base_data, y_value='crs_baseline')        feature_set = {'baseline': (features_x, features_y),                       'crs': (features_x_crs, features_y_crs)}        results = run_in_parallel(run_operator_forecast, # function call                                  base_data, # function args                                  site,                                  [start for start, _ in time_ranges],                                  [end for _, end in time_ranges],                                  feature_set, # forecaster needs features                                  derating_factors,                                  prediction_periods,                                  output_suffix,                                  runs=len(time_ranges),                                  processes=processes)    return results# TODO: Create class for storing standard operator results over many runsclass OperatorResults:    def __init__(self ):        pass