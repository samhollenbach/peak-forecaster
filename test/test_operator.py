from itertools import chainimport datetimeimport randomimport tensorflowimport pandas as pdfrom peak_forecaster import savingsfrom savings import SavingsCalculatorimport osfrom pprint import pprintimport numpy as npimport multiprocessing as mpimport matplotlib.pyplot as pltfrom peak_forecaster import load, process, operate, display, networksolar_sites = ['WM3140', 'WM3796', 'WM5603', 'WM5635', 'WFSTC']# @pickle_jar(verbose=True, reload=True)def do_network_training(train_data_x, train_data_y, test_data_x,                        train_type='baseline'):    print(f"Starting training for: {train_type}")    # Create a network to predict peaks    if train_type == 'baseline':        net = network.Network({'activation': 'relu',                               'layers': 2,                               'learning_rate': 0.001,                               'loss_positive_scalar': 5.0,                               'nodes': [16, 4],                               'optimizer':                                   tensorflow.keras.optimizers.RMSprop})    else:        net = network.Network({'activation': 'relu',                               'layers': 2,                               'learning_rate': 0.001,                               'loss_positive_scalar': 0.8,                               'nodes': [16, 4],                               'optimizer':                                   tensorflow.keras.optimizers.RMSprop})    net.build_model(len(train_data_x[0]), len(train_data_y[0]))    net.train_model(train_data_x, train_data_y)    predictions_crs = net.model.predict(test_data_x)    return predictions_crs    # true_peaks = [y[0] for y in test_y]    # dif = [t - p for t, p in zip(true_peaks, predictions)]    #    # plt.scatter(true_peaks, dif)    # plt.show(site = 'WFLAT'savings_total = []savings_demand = []savings_energy = []# Load database_data = load.load_all_data([site])base_data = base_data.reset_index(drop=True)dis_data = base_data.copy()dis_data['offsets'] = 0dis_data['target'] = 0dis_data['soc'] = 0# display.baseline_plot2(dis_data)print("\n##### Finished Loading Data ##### \n")print("NaNs:")print(base_data.isna().sum())print("Stats:")print(base_data.describe().transpose())print("Tail:")# print(base_data[['timestamp']].head())features_x, features_y = process.extract_features(base_data, y_value='building_baseline')features_x_crs, features_y_crs = process.extract_features(base_data, y_value='crs_baseline')# Compute time ranges# months = range(5, 11)if site == 'WFLAT':    year = '2019'    months = range(5, 8)else:    year = '2018'    months = range(5, 11)days = ['01', '07', '15', '22']time_ranges = [(f'{year}-{m:02d}-{d} 00:00:00-07',                f'{year}-{m+1:02d}-{d} 23:45:00-07') for m in months for d in days]if site == 'WFLAT':    time_ranges = time_ranges[3:-3]def run_operator(base_data, site, start, end, features_x, features_y, features_x_crs, features_y_crs):    print("\nStart: ", start)    print("End: ", end, '\n')    # Split Data    start_date = pd.to_datetime(start).date()    end_date = pd.to_datetime(end).date()    train_data, test_data, is_test = process.train_test_split(base_data,                                                              test_start_date=start_date,                                                              test_end_date=end_date,                                                              return_is_test=True)    perfect_forecasting = True    true_crs = True    if perfect_forecasting:        # Simulate Perfect Forecasting        predictions = []        for t in test_data:            # on_peak = t.loc[(t['timestamp'].dt.time >= datetime.time(12, 0))            #                 & (t['timestamp'].dt.time <= datetime.time(18, 0))]            predictions.append([t['building_baseline'].max()])        print(predictions)        if true_crs:            predictions_crs = []            for tdata, mb in zip(test_data, predictions):                max_baseline = mb[0]                crs_val = tdata.loc[tdata['building_baseline'] == max_baseline]                predictions_crs.append([crs_val['crs_baseline'].values[0]])        else:            predictions_crs = [[max(t['crs_baseline'])] for t in test_data]    else:        # Use forecaster for predictions        train_x = np.array([list(f) for f, t in zip(features_x, is_test) if not t])        train_y = np.array([list(f) for f, t in zip(features_y, is_test) if not t])        test_x = np.array([list(f) for f, t in zip(features_x, is_test) if t])        train_x_crs = np.array([f for f, t in zip(features_x_crs, is_test) if not t])        train_y_crs = np.array([f for f, t in zip(features_y_crs, is_test) if not t])        test_x_crs = np.array([f for f, t in zip(features_x_crs, is_test) if t])        # Use actual forecasting        predictions = do_network_training(train_x, train_y, test_x)        predictions_crs = do_network_training(train_x_crs, train_y_crs, test_x_crs, train_type='crs')    # Define standard operation window    operator = operate.StandardOperator(site, start, end)    # Define daily testing window    tstart = datetime.time(0, 0)    tend = datetime.time(23, 59)    # Run standard operation    targets = operator.run_standard_operation(tstart, tend, test_data,                                              predictions=predictions,                                              crs_predictions=predictions_crs,                                              dynamic_precool=False,                                              threshold_derating=1.0,                                              start_soc=0.5,                                              start_nth_threshold=None)    # Save results    path = f'output/{site}_true_perfect'    if not os.path.exists(path):        os.makedirs(path)    targets.to_pickle(f'{path}/LT_Setback_EBM_'                      f'{site}_{start.split(" ")[0]}_'                      f'{end.split(" ")[0]}.pickle')    mapping_overrides = {'baseline': 'building_baseline',                         'actual': 'target'}    sc = SavingsCalculator('pge_e19_2019', mapping_overrides=mapping_overrides)    breakdown = sc.calculate_savings_breakdown(        targets, group_over=('season', 'period'))['actual_savings_breakdown']['all']    # print(breakdown)    # Display operational data    return targets, breakdownwith mp.Pool(processes=mp.cpu_count()) as pool:    print("Started Pool...")    results = pool.starmap(run_operator,                            zip([base_data] * len(time_ranges),                            [site] * len(time_ranges),                            [start for start, _ in time_ranges],                            [end for _, end in time_ranges],                            [features_x] * len(time_ranges),                            [features_y] * len(time_ranges),                            [features_x_crs] * len(time_ranges),                            [features_y_crs] * len(time_ranges)))    for targets, savings_breakdown in results:        pprint(savings_breakdown)        savings_total.append(savings_breakdown['Total']['Savings'])        savings_demand.append(savings_breakdown['Demand']['Savings'])        savings_energy.append(savings_breakdown['Energy']['Savings'])        print(targets.columns)        print(targets.head())        display.baseline_plot2(targets, savings=(savings_breakdown['Total']['Savings'],                                                 savings_breakdown['Demand']['Savings'],                                                 savings_breakdown['Energy']['Savings']))print("Summer Month Runs")print(f"Net: {savings_total}")print(f"Demand: {savings_demand}")print(f"Energy: {savings_energy}")print("Average Savings")print(f"Net: ${sum(savings_total)/len(savings_total):0.2f}")print(f"Demand: ${sum(savings_demand)/len(savings_demand):0.2f}")print(f"Energy: ${sum(savings_energy)/len(savings_energy):0.2f}")# test_standard_operator()