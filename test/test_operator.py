from itertools import chainimport datetimeimport randomimport tensorflowimport pandas as pdfrom peak_forecaster import savingsimport osfrom pprint import pprintimport numpy as npimport matplotlib.pyplot as pltfrom pickle_jar.pickle_jar import pickle_jarfrom peak_forecaster import load, process, operate, display, networksolar_sites = ['WM3140', 'WM3796', 'WM5603', 'WM5635', 'WFSTC']def test_standard_operator():    site = 'WM3140'    start = '2018-01-01 00:00:00-07'    end = '2018-02-01 00:00:00-07'    #    # start = '2018-01-03 00:00:00-07'    # end = '2018-02-01 23:45:00-07'    targets = load.load_all_data([site])    print("\n##### Finished Loading Data ##### \n")    print("NaNs:")    print(targets.isna().sum())    print("Stats:")    print(targets.describe())    print("Tail:")    print(targets.tail())    # @pickle_jar(verbose=True, reload=True)    def do_data_split(start_date, end_date, data):        # Get train/test data set split based on given dates        train_data, test_data = process.train_test_split(data,                                                         test_start_date=start_date,                                                         test_end_date=end_date)        return train_data, test_data    # @pickle_jar(verbose=True, reload=True)    def do_xy_split(train_data, test_data, y_value='building_baseline'):        # Get x/y features from train and test sets        train_x, train_y = process.extract_features(train_data, y_value=y_value)        test_x, test_y = process.extract_features(test_data, y_value=y_value)        return train_x, train_y, test_x, test_y    # @pickle_jar(verbose=True, reload=True)    def do_network_training(train_data_x, train_data_y, test_data_x,                            train_type='baseline'):        print(f"Starting training for: {train_type}")        # Create a network to predict peaks        if train_type == 'baseline':            net = network.Network({'activation': 'relu',                                   'layers': 2,                                   'learning_rate': 0.001,                                   'loss_positive_scalar': 5.0,                                   'nodes': [16, 4],                                   'optimizer':                                       tensorflow.keras.optimizers.RMSprop})        else:            net = network.Network({'activation': 'relu',                                   'layers': 2,                                   'learning_rate': 0.001,                                   'loss_positive_scalar': 1.0,                                   'nodes': [16, 4],                                   'optimizer':                                       tensorflow.keras.optimizers.RMSprop})        net.build_model(len(train_data_x[0]), len(train_data_y[0]))        net.train_model(train_data_x, train_data_y)        predictions_crs = net.model.predict(test_data_x)        return predictions_crs        # true_peaks = [y[0] for y in test_y]        # dif = [t - p for t, p in zip(true_peaks, predictions)]        #        # plt.scatter(true_peaks, dif)        # plt.show(    runs = 0    savings_total = []    savings_demand = []    savings_energy = []    # Load data    base_data = load.load_all_data([site])    base_data = base_data.reset_index(drop=True)    features_x, features_y = process.extract_features(base_data, y_value='building_baseline')    features_x_crs, features_y_crs = process.extract_features(base_data, y_value='crs_baseline')    # Compute time ranges    months = range(5, 11)    days = ['01', '07', '15', '22']    time_ranges = [(f'2018-{m:02d}-{d} 00:00:00-07',                    f'2018-{m+1:02d}-{d} 23:45:00-07') for d in days for m in months]    # Operation time range    for start, end in time_ranges:        print("\nStart: ", start)        print("End: ", end, '\n')        # Split Data        start_date = pd.to_datetime(start).date()        end_date = pd.to_datetime(end).date()        train_data, test_data, is_test = process.train_test_split(base_data,                                                                  test_start_date=start_date,                                                                  test_end_date=end_date,                                                                  return_is_test=True)        perfect_forecasting = True        true_crs = True        if perfect_forecasting:            # Simulate Perfect Forecasting            predictions = [[max(t['building_baseline'])] for t in test_data]            if true_crs:                predictions_crs = []                for mb in predictions:                    max_baseline = mb[0]                    crs_val = test_data.loc[test_data['building_baseline'] == max_baseline]['crs_baseline']                    predictions_crs.append(crs_val)            else:                predictions_crs = [[max(t['crs_baseline'])] for t in test_data]        else:            # Use forecaster for predictions            train_x = np.array([list(f) for f, t in zip(features_x, is_test) if not t])            train_y = np.array([list(f) for f, t in zip(features_y, is_test) if not t])            test_x = np.array([list(f) for f, t in zip(features_x, is_test) if t])            train_x_crs = np.array([f for f, t in zip(features_x_crs, is_test) if not t])            train_y_crs = np.array([f for f, t in zip(features_y_crs, is_test) if not t])            test_x_crs = np.array([f for f, t in zip(features_x_crs, is_test) if t])            # Use actual forecasting            predictions = do_network_training(train_x, train_y, test_x)            predictions_crs = do_network_training(train_x_crs, train_y_crs, test_x_crs, train_type='crs')        # Define standard operation window        operator = operate.StandardOperator(site, start, end)        # Define daily testing window        tstart = datetime.time(3, 0)        tend = datetime.time(22, 0)        # Run standard operation        targets = operator.run_standard_operation(tstart, tend, test_data,                                                  predictions=predictions,                                                  crs_predictions=predictions_crs,                                                  dynamic_precool=False,                                                  threshold_derating=1.0,                                                  start_soc=0.5,                                                  start_nth_threshold=1)        # Save results        path = f'output/{site}_1_hottest_true_crs'        if not os.path.exists(path):            os.makedirs(path)        targets.to_pickle(f'{path}/LT_Setback_EBM_'                          f'{site}_{start.split(" ")[0]}_'                          f'{end.split(" ")[0]}.pickle')        # Calculate operational savings        s = savings.Savings('pge_e19_2019')        baseline = targets[['timestamp', 'building_baseline']]        target = targets[['timestamp', 'target']]        total_savings = s.get_savings(baseline, target)        demand_savings = s.get_demand_savings(baseline, target)        energy_savings = s.get_energy_savings(baseline, target)        savings_total.append(total_savings)        savings_demand.append(demand_savings)        savings_energy.append(energy_savings)        print(f"Savings: ${total_savings:.2f}")        print(f"Demand Savings: ${demand_savings:.2f}")        print(f"Energy Savings: ${energy_savings:.2f}\n")        # Display operational data        display.baseline_plot2(targets, savings=(total_savings, demand_savings, energy_savings))        runs += 1    print("Summer Month Runs")    print(f"Net: {savings_total}")    print(f"Demand: {savings_demand}")    print(f"Energy: {savings_energy}")    print("Average Savings")    print(f"Net: ${sum(savings_total)/runs:0.2f}")    print(f"Demand: ${sum(savings_demand)/runs:0.2f}")    print(f"Energy: ${sum(savings_energy)/runs:0.2f}")test_standard_operator()